{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Average CV RMS Error: 160216.6542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "data_file = '../numerical_data.csv'\n",
    "df = pd.read_csv(data_file, index_col='id')\n",
    "\n",
    "# Ensure the target exists\n",
    "if 'CompTotal' not in df.columns:\n",
    "    raise ValueError(\"Target column 'CompTotal' not found in dataset.\")\n",
    "\n",
    "# Cap extreme outliers for the target\n",
    "threshold = 1e6\n",
    "df['CompTotal'] = np.clip(df['CompTotal'], 0, threshold)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('CompTotal', axis=1)\n",
    "y = df['CompTotal']\n",
    "\n",
    "# Automatically identify categorical and numerical columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Create a preprocessor that scales numerical features and one-hot encodes categorical features.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build a pipeline with a basic linear regression model.\n",
    "# The TransformedTargetRegressor applies np.log1p to the target during training and np.expm1 when predicting.\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "model = TransformedTargetRegressor(regressor=pipeline, func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "# 5-fold Cross-Validation to evaluate model performance\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rms_errors = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    rms_errors.append(rms)\n",
    "\n",
    "avg_rms = np.mean(rms_errors)\n",
    "print(f\"Model Average CV RMS Error: {avg_rms:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (b) Feature Engineering with K-Means Clustering ===\n",
      "Feature Engineered Model Average CV RMS Error: 145013.7387\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# -----------------------\n",
    "# Data Loading & Cleaning\n",
    "# -----------------------\n",
    "data_file = '../numerical_data.csv'\n",
    "df = pd.read_csv(data_file, index_col='id')\n",
    "\n",
    "if 'CompTotal' not in df.columns:\n",
    "    raise ValueError(\"Target column 'CompTotal' not found in dataset.\")\n",
    "\n",
    "# Cap extreme outliers for the target and reset index\n",
    "threshold = 1e6\n",
    "df['CompTotal'] = np.clip(df['CompTotal'], 0, threshold)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('CompTotal', axis=1)\n",
    "y = df['CompTotal']\n",
    "\n",
    "# -----------------------\n",
    "# Preprocessing: One-Hot Encoding & Scaling\n",
    "# -----------------------\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Create a preprocessor:\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor so that all sub-transformers are fitted.\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Retrieve the OneHotEncoder instance from the preprocessor.\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "\n",
    "# In case the OneHotEncoder isn't fitted (it should be after preprocessor.fit_transform),\n",
    "# you can force a fit on just the categorical data:\n",
    "if not hasattr(cat_encoder, 'categories_'):\n",
    "    cat_encoder.fit(X[categorical_features])\n",
    "\n",
    "# Now, get the feature names for the categorical part.\n",
    "cat_features_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine numerical feature names (unchanged) with categorical feature names.\n",
    "all_feature_names = list(numerical_features) + list(cat_features_names)\n",
    "\n",
    "# Convert transformed features into a DataFrame.\n",
    "X_preprocessed = pd.DataFrame(\n",
    "    X_transformed.toarray() if hasattr(X_transformed, \"toarray\") else X_transformed,\n",
    "    columns=all_feature_names\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Part (b): Feature Engineering via K-Means Clustering\n",
    "# -----------------------\n",
    "print(\"\\n=== (b) Feature Engineering with K-Means Clustering ===\")\n",
    "\n",
    "# Apply K-Means clustering on the preprocessed features.\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_preprocessed)\n",
    "\n",
    "# Convert cluster labels into dummy variables.\n",
    "cluster_dummies = pd.get_dummies(clusters, prefix=\"cluster\")\n",
    "\n",
    "# Create an enriched feature set by combining preprocessed features with cluster dummies.\n",
    "X_enriched = pd.concat([X_preprocessed, cluster_dummies], axis=1)\n",
    "\n",
    "# Apply log1p transformation to the target for variance stabilization.\n",
    "y_trans = np.log1p(y)\n",
    "\n",
    "# 5-fold Cross-Validation for the feature-engineered model.\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fe_rms_errors = []\n",
    "fe_model_params = []  # Save model parameters for each fold\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_enriched):\n",
    "    X_train, X_val = X_enriched.iloc[train_idx], X_enriched.iloc[val_idx]\n",
    "    y_train, y_val = y_trans.iloc[train_idx], y_trans.iloc[val_idx]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds_trans = model.predict(X_val)\n",
    "    preds = np.expm1(preds_trans)  # Invert the log1p transformation\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_val_orig, preds))\n",
    "    fe_rms_errors.append(rms)\n",
    "    fe_model_params.append({'coefficients': model.coef_, 'intercept': model.intercept_})\n",
    "\n",
    "avg_rms_fe = np.mean(fe_rms_errors)\n",
    "print(f\"Feature Engineered Model Average CV RMS Error: {avg_rms_fe:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (c) Regularized Model with Expanded Basis Functions ===\n",
      "Best Ridge Model Alpha: 100\n",
      "Best Ridge Model Test RMS Error: 137276.6711\n",
      "Norm of Best Ridge Model Parameters: 2.2193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinaghadimi/anaconda3/envs/project-133A/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:215: LinAlgWarning: Ill-conditioned matrix (rcond=8.5967e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"\\n=== (c) Regularized Model with Expanded Basis Functions ===\")\n",
    "data_file = '../numerical_data.csv'\n",
    "df = pd.read_csv(data_file, index_col='id')\n",
    "if 'CompTotal' not in df.columns:\n",
    "    raise ValueError(\"Target column 'CompTotal' not found in dataset.\")\n",
    "\n",
    "# Cap extreme outliers\n",
    "threshold = 1e6\n",
    "df['CompTotal'] = np.clip(df['CompTotal'], 0, threshold)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define features and target; apply log1p transform to the target.\n",
    "X = df.drop('CompTotal', axis=1)\n",
    "y = df['CompTotal']\n",
    "y_trans = np.log1p(y)\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "X_clean = X_encoded\n",
    "\n",
    "# Expand features using 2nd order polynomial expansion (without bias)\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly2 = pd.DataFrame(poly2.fit_transform(X_clean), columns=poly2.get_feature_names_out(X_clean.columns))\n",
    "\n",
    "# Generate clusters using KMeans on the one-hot encoded features\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_clean)\n",
    "cluster_dummies = pd.get_dummies(clusters, prefix=\"cluster\")\n",
    "\n",
    "# Combine polynomial features with cluster dummies\n",
    "X_c = pd.concat([X_poly2, cluster_dummies], axis=1)\n",
    "\n",
    "# Split data into training and testing sets (using y_trans as target)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, y_trans, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate Ridge regression over a set of alphas\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "ridge_rms_errors = []\n",
    "ridge_models = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge.fit(X_train_c, y_train_c)\n",
    "    preds_test_trans = ridge.predict(X_test_c)\n",
    "    preds_test = np.expm1(preds_test_trans)\n",
    "    y_test_orig = np.expm1(y_test_c)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_test_orig, preds_test))\n",
    "    ridge_rms_errors.append(rms)\n",
    "    ridge_models.append(ridge)\n",
    "\n",
    "best_alpha_idx = np.argmin(ridge_rms_errors)\n",
    "best_alpha = alphas[best_alpha_idx]\n",
    "best_ridge = ridge_models[best_alpha_idx]\n",
    "best_ridge_rms = ridge_rms_errors[best_alpha_idx]\n",
    "coef_norm = np.linalg.norm(best_ridge.coef_)\n",
    "\n",
    "print(f\"Best Ridge Model Alpha: {best_alpha}\")\n",
    "print(f\"Best Ridge Model Test RMS Error: {best_ridge_rms:.4f}\")\n",
    "print(f\"Norm of Best Ridge Model Parameters: {coef_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (d) Non-Linear Model using Degree-3 Polynomial Expansion ===\n",
      "Non-Linear (Degree-3) Model Average CV RMS Error: 125609342.4648\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== (d) Non-Linear Model using Degree-3 Polynomial Expansion ===\")\n",
    "\n",
    "poly3 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly3 = pd.DataFrame(poly3.fit_transform(X_clean), columns=poly3.get_feature_names_out(X_clean.columns))\n",
    "\n",
    "# Combine the degree-3 polynomial features with the cluster dummies (re-use same clusters)\n",
    "X_d = pd.concat([X_poly3, cluster_dummies], axis=1)\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "nonlinear_rms_errors = []\n",
    "nonlinear_model_params = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_d):\n",
    "    X_train, X_val = X_d.iloc[train_idx], X_d.iloc[val_idx]\n",
    "    y_train, y_val = y_trans.iloc[train_idx], y_trans.iloc[val_idx]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_trans = model.predict(X_val)\n",
    "    \n",
    "    # Clip predictions in log-space to prevent extreme outputs\n",
    "    preds_trans = np.clip(preds_trans, -20, 20)\n",
    "    preds = np.expm1(preds_trans)\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_val_orig, preds))\n",
    "    nonlinear_rms_errors.append(rms)\n",
    "    nonlinear_model_params.append({'coefficients': model.coef_, 'intercept': model.intercept_})\n",
    "\n",
    "avg_rms_nonlinear = np.mean(nonlinear_rms_errors)\n",
    "print(f\"Non-Linear (Degree-3) Model Average CV RMS Error: {avg_rms_nonlinear:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
