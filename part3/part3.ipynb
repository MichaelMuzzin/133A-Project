{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (a) Basic Linear Model ===\n",
      "Basic Linear Model Average CV RMS Error: 160216.6542\n",
      "\n",
      "=== (b) Feature Engineering with K-Means Clustering ===\n",
      "Feature Engineered Model Average CV RMS Error: 145051.3020\n",
      "\n",
      "=== (c) Regularized Model with Expanded Basis Functions ===\n",
      "Best Ridge Model Alpha: 100\n",
      "Best Ridge Model Test RMS Error: 137528.3966\n",
      "Norm of Best Ridge Model Parameters: 3.4383\n",
      "\n",
      "=== (d) Non-Linear Model using Degree-3 Polynomial Expansion ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# =============================================================================\n",
    "# Data Loading and Preliminary Outlier Handling\n",
    "# =============================================================================\n",
    "# Load the cleaned CSV (combed through numerical_data.csv)\n",
    "data_file = '../numerical_data.csv'\n",
    "df = pd.read_csv(data_file, index_col='id')\n",
    "\n",
    "# Ensure the target exists\n",
    "if 'CompTotal' not in df.columns:\n",
    "    raise ValueError(\"Target column 'CompTotal' not found in dataset.\")\n",
    "\n",
    "# (Optional) Winsorize extreme outliersâ€”here we cap CompTotal at 1e6.\n",
    "# You can adjust the threshold as needed.\n",
    "threshold = 1e6\n",
    "df['CompTotal'] = np.clip(df['CompTotal'], 0, threshold)\n",
    "\n",
    "# Reset the index to avoid alignment issues later.\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# Part (a): Basic Linear Model\n",
    "# =============================================================================\n",
    "# Use standardized features only (no additional transformations)\n",
    "X_a = df.drop('CompTotal', axis=1)\n",
    "y = df['CompTotal']\n",
    "\n",
    "# Transform target using log1p to stabilize variance\n",
    "y_trans = np.log1p(y)\n",
    "\n",
    "# Standardize features and impute missing values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_a), columns=X_a.columns)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_clean = pd.DataFrame(imputer.fit_transform(X_scaled), columns=X_scaled.columns)\n",
    "\n",
    "# 5-fold Cross-Validation for basic linear model\n",
    "print(\"=== (a) Basic Linear Model ===\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "basic_rms_errors = []\n",
    "basic_model_params = []  # list to save model parameters per fold\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_clean):\n",
    "    X_train, X_val = X_clean.iloc[train_idx], X_clean.iloc[val_idx]\n",
    "    y_train, y_val = y_trans.iloc[train_idx], y_trans.iloc[val_idx]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_trans = model.predict(X_val)\n",
    "    preds = np.expm1(preds_trans)  # invert log transform\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_val_orig, preds))\n",
    "    basic_rms_errors.append(rms)\n",
    "    basic_model_params.append({'coefficients': model.coef_, 'intercept': model.intercept_})\n",
    "\n",
    "avg_rms_basic = np.mean(basic_rms_errors)\n",
    "print(f\"Basic Linear Model Average CV RMS Error: {avg_rms_basic:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Part (b): Feature Engineering via K-Means Clustering\n",
    "# =============================================================================\n",
    "print(\"\\n=== (b) Feature Engineering with K-Means Clustering ===\")\n",
    "# Apply k-means clustering on X_clean\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_clean)\n",
    "cluster_dummies = pd.get_dummies(clusters, prefix=\"cluster\")\n",
    "\n",
    "# Create an enriched feature set by combining original features with cluster dummies\n",
    "X_b = pd.concat([X_clean, cluster_dummies], axis=1)\n",
    "\n",
    "fe_rms_errors = []\n",
    "fe_model_params = []  # Save parameters for each fold\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_b):\n",
    "    X_train, X_val = X_b.iloc[train_idx], X_b.iloc[val_idx]\n",
    "    y_train, y_val = y_trans.iloc[train_idx], y_trans.iloc[val_idx]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_trans = model.predict(X_val)\n",
    "    preds = np.expm1(preds_trans)\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_val_orig, preds))\n",
    "    fe_rms_errors.append(rms)\n",
    "    fe_model_params.append({'coefficients': model.coef_, 'intercept': model.intercept_})\n",
    "\n",
    "avg_rms_fe = np.mean(fe_rms_errors)\n",
    "print(f\"Feature Engineered Model Average CV RMS Error: {avg_rms_fe:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Part (c): Regularized Model with Expanded Basis Functions (Ridge Regression)\n",
    "# =============================================================================\n",
    "print(\"\\n=== (c) Regularized Model with Expanded Basis Functions ===\")\n",
    "# Expand features using 2nd order polynomial expansion (without bias)\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly2 = pd.DataFrame(poly2.fit_transform(X_clean), columns=poly2.get_feature_names_out(X_clean.columns))\n",
    "\n",
    "# Combine polynomial features with cluster dummies\n",
    "X_c = pd.concat([X_poly2, cluster_dummies], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, y_trans, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate Ridge regression over a set of alphas\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "ridge_rms_errors = []\n",
    "ridge_models = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge.fit(X_train_c, y_train_c)\n",
    "    preds_test_trans = ridge.predict(X_test_c)\n",
    "    preds_test = np.expm1(preds_test_trans)\n",
    "    y_test_orig = np.expm1(y_test_c)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_test_orig, preds_test))\n",
    "    ridge_rms_errors.append(rms)\n",
    "    ridge_models.append(ridge)\n",
    "\n",
    "best_alpha_idx = np.argmin(ridge_rms_errors)\n",
    "best_alpha = alphas[best_alpha_idx]\n",
    "best_ridge = ridge_models[best_alpha_idx]\n",
    "best_ridge_rms = ridge_rms_errors[best_alpha_idx]\n",
    "coef_norm = np.linalg.norm(best_ridge.coef_)\n",
    "\n",
    "print(f\"Best Ridge Model Alpha: {best_alpha}\")\n",
    "print(f\"Best Ridge Model Test RMS Error: {best_ridge_rms:.4f}\")\n",
    "print(f\"Norm of Best Ridge Model Parameters: {coef_norm:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Part (d): Non-Linear Data Fitting using Degree-3 Polynomial Expansion\n",
    "# =============================================================================\n",
    "print(\"\\n=== (d) Non-Linear Model using Degree-3 Polynomial Expansion ===\")\n",
    "poly3 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly3 = pd.DataFrame(poly3.fit_transform(X_clean), columns=poly3.get_feature_names_out(X_clean.columns))\n",
    "X_d = pd.concat([X_poly3, cluster_dummies], axis=1)\n",
    "\n",
    "nonlinear_rms_errors = []\n",
    "nonlinear_model_params = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_d):\n",
    "    X_train, X_val = X_d.iloc[train_idx], X_d.iloc[val_idx]\n",
    "    y_train, y_val = y_trans.iloc[train_idx], y_trans.iloc[val_idx]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_trans = model.predict(X_val)\n",
    "    # Clip predictions in log-space to prevent extreme outputs\n",
    "    preds_trans = np.clip(preds_trans, -20, 20)\n",
    "    preds = np.expm1(preds_trans)\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    \n",
    "    rms = np.sqrt(mean_squared_error(y_val_orig, preds))\n",
    "    nonlinear_rms_errors.append(rms)\n",
    "    nonlinear_model_params.append({'coefficients': model.coef_, 'intercept': model.intercept_})\n",
    "\n",
    "avg_rms_nonlinear = np.mean(nonlinear_rms_errors)\n",
    "print(f\"Non-Linear (Degree-3) Model Average CV RMS Error: {avg_rms_nonlinear:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Part (e): Final Predictions and Report\n",
    "# =============================================================================\n",
    "print(\"\\n=== (e) Final Predictions on Unseen Data ===\")\n",
    "# For demonstration, use a held-out test set.\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X_c, y_trans, test_size=0.2, random_state=42)\n",
    "\n",
    "# Here, we choose the best Ridge model from Part (c) as our final predictor.\n",
    "final_model = best_ridge\n",
    "final_preds_trans = final_model.predict(X_test_final)\n",
    "final_preds = np.expm1(final_preds_trans)\n",
    "y_test_final_orig = np.expm1(y_test_final)\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test_final_orig, final_preds))\n",
    "print(f\"Final Model (Ridge) Test RMS Error: {final_rmse:.4f}\")\n",
    "\n",
    "# Show sample predictions (first 5 rows of test set)\n",
    "print(\"\\nSample Predictions on Unseen Test Data:\")\n",
    "sample_unseen = X_test_final.iloc[:5]\n",
    "sample_preds_trans = final_model.predict(sample_unseen)\n",
    "sample_preds = np.expm1(sample_preds_trans)\n",
    "for i, pred in enumerate(sample_preds, start=1):\n",
    "    print(f\"Prediction {i}: ${pred:,.2f}\")\n",
    "\n",
    "# Plot Predicted vs Actual for Final Model\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test_final_orig, final_preds, alpha=0.5)\n",
    "plt.plot([0, threshold], [0, threshold], 'r--')\n",
    "plt.xlabel(\"Actual CompTotal\")\n",
    "plt.ylabel(\"Predicted CompTotal\")\n",
    "plt.title(\"Final Model: Predicted vs Actual Developer Income\")\n",
    "plt.xlim(0, threshold)\n",
    "plt.ylim(0, threshold)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Final Report (Markdown Format)\n",
    "# -----------------------------------------------------------------------------\n",
    "report = f\"\"\"\n",
    "# Model Evaluation and Prediction Report\n",
    "\n",
    "**Due Date:** March 9, 2025  \n",
    "**Weight:** 25% of the total 50%\n",
    "\n",
    "## Overview\n",
    "This project predicts developer income (**CompTotal**) using a combed-through dataset (numerical_data.csv). We:\n",
    "- **(a)** Built a basic linear model using only standardized features with 5-fold cross-validation.\n",
    "- **(b)** Enhanced features by adding k-means clustering (k=3) as one-hot encoded variables.\n",
    "- **(c)** Applied a regularized Ridge regression model with 2nd order polynomial expansion and tuned the regularization parameter.\n",
    "- **(d)** Explored a non-linear model using a 3rd order polynomial expansion.\n",
    "- **(e)** Produced final predictions on unseen test data using the best Ridge model.\n",
    "\n",
    "## (a) Basic Linear Model\n",
    "- **Average CV RMS Error:** {avg_rms_basic:.4f}  \n",
    "- **Model Parameters:** Saved per fold.\n",
    "\n",
    "## (b) Feature Engineering with K-Means\n",
    "- **Average CV RMS Error:** {avg_rms_fe:.4f}  \n",
    "- **Model Parameters:** Saved per fold.\n",
    "\n",
    "## (c) Regularized Ridge Model with Expanded Basis Functions\n",
    "- **Best Alpha:** {best_alpha}  \n",
    "- **Test RMS Error:** {best_ridge_rms:.4f}  \n",
    "- **Parameter Norm:** {coef_norm:.4f}\n",
    "\n",
    "## (d) Non-Linear Model (Degree-3 Polynomial Expansion)\n",
    "- **Average CV RMS Error:** {avg_rms_nonlinear:.4f}  \n",
    "- **Model Parameters:** Saved per fold.\n",
    "\n",
    "## (e) Final Predictions\n",
    "- **Final Model:** Ridge Regression from (c)\n",
    "- **Final Test RMS Error:** {final_rmse:.4f}  \n",
    "- **Sample Predictions:** {', '.join(f'${p:,.2f}' for p in sample_preds)}\n",
    "\n",
    "## Discussion\n",
    "The basic linear model yielded an RMS error of approximately {avg_rms_basic:.4f}. Enhancing the feature set with k-means clustering reduced the error to {avg_rms_fe:.4f}. Incorporating polynomial expansion and applying Ridge regression improved generalization (Test RMS Error: {best_ridge_rms:.4f}, Parameter Norm: {coef_norm:.4f}). A degree-3 non-linear model, however, was unstable, with an average RMS error of {avg_rms_nonlinear:.4f}. Overall, the best performance was achieved with the Ridge model, which we used for final predictions.\n",
    "\n",
    "## Conclusion\n",
    "While our baseline and feature-engineered models showed moderate improvements, regularization combined with expanded basis functions provided a more robust and accurate prediction of developer income. Future work may explore additional non-linear models or alternative feature engineering strategies (e.g., incorporating domain-specific variables like \"Time Spent in Software\") for further improvements.\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
